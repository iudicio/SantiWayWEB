# Anomaly Detector (Backend) — 1.0.0‑beta

REST API и Python‑библиотека для детекции аномалий в больших табличных данных.
В основе — **IsolationForest** (scikit‑learn) со встроенной очисткой данных,
нормализацией, батч‑обработкой, калибровкой порогов и удобными вероятностями
аномальности.


##  Структура проекта

```
anomaly-detector/
├── src/
│   └── anomaly_detector/
│       ├── __init__.py
│       └── detector.py          # BackendAnomalyDetector
├── api/
│   └── main.py                  # FastAPI-обёртка
├── tests/
│   └── test_detector.py         # базовые pytest-тесты
├── README.md
```

##  Возможности

- Поддержка **NumPy** и **pandas DataFrame** как входных данных.
- Очистка: импьютация `NaN/Inf` (медиана), стандартизация с `StandardScaler`.
- **Калибровка порогов** серьёзности (`LOW/MEDIUM/HIGH/CRITICAL`) по обучающей выборке.
- Оценка **вероятности аномальности** через эмпирическую CDF.
- Батч‑обработка больших массивов (`predict_batch`).
- Потокобезопасная статистика вызовов (опция `thread_safe=True`).
- Сохранение/загрузка модели (`joblib`) c проверкой версий.
- Готовый **FastAPI** REST‑слой с CORS и Pydantic‑схемами.
- Набор **pytest**‑тестов.

##  Требования

- Python **3.9+** (рекомендуется 3.10–3.12)
- Библиотеки:
  - `scikit-learn`, `numpy`, `pandas`, `joblib`
  - `fastapi`, `uvicorn`, `pydantic`
  - `pytest` (для тестов)


##  Быстрый старт (API)

Запуск сервера:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

Проверка статуса:

```bash
curl http://localhost:8000/
curl http://localhost:8000/health
```

### 1) Обучение модели — `POST /train`

Тело запроса может быть:
- **Словарь столбцов** (`Dict[str, List[float]]`) — предпочтительно для DataFrame.
- **Список строк** (`List[List[float]]`) — эквивалент `ndarray`.

> **Важно (по умолчанию policy='strict'):**
> если модель обучена на DataFrame, то **детекция тоже требует DataFrame с теми же именами столбцов**.
> Для более гибкого поведения можно создать детектор с `feature_names_policy="flexible"`.



### 2) Детекция — `POST /detect`

Поля:
- `data`: те же два формата, что и для обучения.
- `threshold`: *необязательный* кастомный порог по score (если не задан — используется предсказание модели).
- `return_details`: `true|false` — вернуть ли подробности по каждой точке.

### 3) Обучение модели из CSV — `POST /train/csv`

Тело запроса — `multipart/form-data` с файлом:
- поле: **`file`** — CSV-файл.

Параметры (query):
- `has_header`: `true|false` — есть ли строка заголовков (по умолчанию `true`).
- `delimiter`: разделитель (если не задан — авто-детект из `, ; \t |`).
- `decimal`: `.` или `,` — десятичный разделитель (по умолчанию `.`).
- `encoding`: кодировка (`utf-8` по умолчанию).
- `na_values`: строка с NA-токенами через запятую (например, `NA,NULL,?`).
- `validation_split`: доля валидации, `[0,1)`, по умолчанию `0.2`.
- `contamination`: доля аномалий, `(0,0.5)`, по умолчанию `0.1`.
- `n_estimators`: число деревьев, по умолчанию `100`.
- `sample_rows`: если задано — случайно выбрать `N` строк для обучения.
- `feature_names_policy`: `"strict"` (по умолчанию) или `"flexible"`.

> **Важно (policy='strict' по умолчанию):**  
> модель запоминает **имена столбцов** из CSV. При детекции CSV должен содержать **те же** колонки (порядок не важен).  
> Для более гибкого поведения используйте `feature_names_policy="flexible"` (недостающие — заполнятся `0.0`, лишние — игнорируются).



### 4) Детекция из CSV — `POST /detect/csv`

Тело запроса — `multipart/form-data` с файлом:
- поле: **`file`** — CSV-файл.

Параметры (query):
- `has_header`, `delimiter`, `decimal`, `encoding`, `na_values` — как в `/train/csv`.
- `threshold`: *необязательный* кастомный порог по score (если не задан — используется встроенный в модели).
- `return_details`: `true|false` — возвращать ли подробности по каждой строке (по умолчанию `true`).
- `chunksize`: размер чанка для потоковой обработки больших CSV (по умолчанию `50000`).

**Рекомендация для больших файлов:**  
Если нужны только агрегаты — вызывайте с `return_details=false` и увеличивайте `chunksize` (например, `200000–500000`) при достаточной памяти.


### 5) Служебные эндпоинты

- `GET /model/info` — полная информация о текущей модели (порогах, распределениях, размерах, статистике).
- `GET /stats` — агрегированная статистика использования (кол-во предсказаний/аномалий и т.д.).
- `POST /stats/reset` — сброс статистики.

##  Использование как библиотеки (Python)

```python
from anomaly_detector import BackendAnomalyDetector
import numpy as np
import pandas as pd

# 1) Инициализация (по умолчанию policy='strict')
det = BackendAnomalyDetector(
    contamination=0.1,
    n_estimators=100,
    thread_safe=False,             # можно True если общий объект в многопоточке
    feature_names_policy='strict'  
)

# 2) Обучение
X = np.random.randn(1000, 5)
det.fit(X)

# 3) Детекция
scores = det.anomaly_scores(X[:10])
probas = det.predict_proba_anomaly(X[:10])
details = det.get_anomaly_details(X[:10])

# 4) Кастомный порог
preds = det.predict_anomalies(X[:10], threshold=-0.15)

# 5) Батч‑обработка больших наборов
det.batch_size = 512
big_results = det.predict_batch(np.random.randn(100_000, 5))

# 6) Сохранение/загрузка
det.save_model("model.joblib")
new_det = BackendAnomalyDetector()
new_det.load_model("model.joblib")
```

##  Как интерпретировать выходы

- `anomaly_score` — «сырое» значение `IsolationForest.decision_function`: **меньше 0 ⇒ вероятнее аномалия**.
- `anomaly_probability` — калиброванное значение из `[0,1]` (чем больше, тем аномальнее). По умолчанию строится эмпирическая CDF по train‑score и инвертируется.
- `severity` — уровень серьёзности, калиброванный на основе распределения train‑score именно **на отрицательных (аномальных) скор‑значениях**.
- `confidence` — эвристика уверенности: расстояние от медианы CDF или скейлинг score.

### Параметр `feature_names_policy`

- `strict` (по умолчанию): строгая проверка состава и порядка столбцов. Удобно для стабильных пайплайнов.
- `flexible`: на инференсе DataFrame будет **переупорядочен** и **дополнен отсутствующими признаками нулями** согласно `feature_names`, полученным при обучении.

##  Производительность и эксплуатация

- `IsolationForest(..., n_jobs=-1)` — автоматическое использование всех доступных ядер на этапе обучения/инференса.
- **Батчи**: для массивов в десятки/сотни тысяч строк используйте `predict_batch`, подберите `batch_size` по памяти/CPU.
- **Параметр `contamination`** должен отражать ожидаемую долю аномалий в данных. Слишком высокий/низкий `contamination` ухудшает качество.
- **Потокобезопасность**: если один объект детектора разделяется между потоками/воркерами — инициализируйте с `thread_safe=True` (уже используется в API).

## Валидация и частые ошибки

- **"Модель не обучена. Сначала вызовите /train"** — перед `POST /detect` обязательна инициализация через `POST /train`.
- **"Отсутствуют признаки: {...}"** — при policy=`strict` имена столбцов на детекции должны **в точности** совпадать с обучением.
- **NaN/Inf** — библиотека заменит на `NaN` и проимпьютирует медианой, но лучше чистить данные заранее.
- **Количество признаков изменилось** — для `ndarray` и policy=`strict` число столбцов должно совпадать с обучением.






